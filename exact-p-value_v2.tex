%\documentclass[journal=jpr,manuscript=article]{achemso}
\documentclass{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{mathtools,amssymb}
\usepackage{dsfont}
\usepackage{color}
\usepackage[english]{babel}
\usepackage{authblk}
\usepackage{url}
%\usepackage{soul}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 

%\graphicspath{{figs/}}  Store figures at the level of the main doc
\newcommand{\argmax}{\text{argmax}}
\newcommand{\todo}[2]{{\color{red} {\bf TODO-#1}: #2}}
\newcommand{\comment}[2]{{\color{blue} {\bf Comment-#1}: #2}}
\newcommand{\correction}[2]{{\color{red} \st{#1} }{\color{red} #2}}
%\newcommand{\correction}[2]{#2}  % Use this to make a document with hiding track changes.

%\usepackage[backend=biber,style=nature,articletitle=false]{biblatex}
%\usepackage[backend=biber,style=nature]{biblatex}
%\addbibresource{bibliography.bib}

%\usepackage[backend=biber]{biblatex}
%\usepackage{natbib}
%\addbibresource{bibliography.bib}
\newcommand{\pe}[0]{\mathrel{{+}{=}}}
\newcommand{\mathone}{\mathds{1}}

\usepackage{setspace}
%\doublespacing
%\newcommand*{\DOUBLEBLINDREVIEW}{} %Uncomment to hide author information for double-blind peer review


\author{Andrey Borevsky}
\author{Attila Kertesz-Farkas$\ast$}
\affil{Laboratory on AI for Computational Biology, Faculty of Computer Science, HSE University,  11 Pokrovsky Bvld., Moscow 109028, Russian Federation}
%\email{akerteszfarkas@hse.ru}
%\phone{+7 (499) 152-07-41}

\title{Towards reliable false discovery rate control in classification problems under distribution shift}
\begin{document}

\maketitle

\begin{abstract}
	Domain shifts or batch effects can significantly degrade the performance of machine learning-based classifiers in software production without any warning signal. This can particularly be problematic in medical applications which involve human lives. The shift changes the shape of the prediction score distribution of the test samples compared to that of the training samples; hence, decision boundary re-calibration would be required to conform the expected performance standards. We present a simple, yet robust heuristic method to mitigate the bias in false discovery rate (FDR) control originating from data shift. Our method is based on an adjustment of the test prediction scores so that test null distribution (approximated distribution of the prediction scores of negative test samples) aligns to the training null distribution (actual distribution of prediction scores of the negative training samples), thereby we termed it Test Null Adjustment (TNA) method. Then, accurate empirical p-values of the adjusted test prediction scores can be calculated with respect to the training null distribution to be used in the Benjamini-Hochberg (BH) protocol for FDR control. The main advantage of the TNA method is that it's application does not require any domain specific knowledge, it works with multi-class scenario, it is fully data-driven, and it operates in the 1-dimensional space of the prediction scores ensuring stable numerical calculations. We have performed four experimental tests on biomedical image analysis domain to demonstrate that the TNA method is able to mitigate the bias in FDR induced by data shift. 
	
	%In this paper, we present a simple to accurately estimate and control the false discovery rate (FDR) on the test data without using class labels which is also robust to domain shifts. Our technique is based on valid, i.e. uniformly distributed, empirical p-values derived from the distribution of the prediction scores of the negative data, i.e. the null-distribution, produced by the classifier. Then, the FDR can be accurately controlled with, for instance, Benjamini-Hochberg protocol or similar ones. Domain shits can be visually detected by comparing the null distributions of the train and test sets with Q-Q plots. In case of domain shift, simply by re-scaling the test null distribution to the train one provides  accurate FDR control. We compared our method with the Learn-Then-Test approach recently developed by Michael Jordan's lab and our experimental results shows that our technique is more robust to data- and class-label distribution shits. 
	
	 
%Artificial Intelligence has been demonstrated as an incredibly useful instrument for a broad range of tasks. One of them — Bioinformatics — proposed fundamentally novel techniques at the intersection of machine learning and statistics. Despite their potential, these methods have not been utilized for more general tasks yet. Accordingly, in our work, we elaborate a drastically new approach called empirical p-values (EPV). Assuming negative training data of classification task to be the null hypothesis distribution, we calculate the corresponding p-values for the test samples. Later, we expand the BH procedure to control FDR, making it possible both to regulate the interrelation of train and test data distributions, as well as to predict new labels based on those already investigated. The major goal is to accurately predict number of accepted discoveries at each level without true labels.


\end{abstract}
\textbf{Key words:} Empirical P-values, False Discovery Rate, Data Shift, Batch Effect

\section{Introduction}

Modern AI applications not only do they need to increase their discriminative power to reduce the overall number of incorrect predictions, but they also need to simultaneously minimize a quantified risks of false positive (FP) and negative (FN) error by an optimal decision threshold calibration while also being able to handle (or detect) data distribution shift \cite{feng2022clinical, al2023artificial}. False positive (type I) and false negative (type II) errors can have significantly different costs or impacts on people and society \cite{wynants2019three}. For instance, a false positive diagnoses of HIV, COVID-19, and cancer can cause emotional trauma and the person may undergo invasive treatments with undesirable complications and side effects \cite{newman2021rate, salz2010meta, Tosteson2014Consequences, xu2016frequency} until a more elaborated clinical test establishes the correct diagnosis. However; a falsely diagnosed person with infectious disease may lead to a false sense of safety \cite{mouliou2021false}, may put not only their but other peoples' life in danger \cite{woloshin2020false}, an undetected cancer may prolong and hinder treatment \cite{bradley2021interpreting}, misdiagnosed human papillomavirus (HPV) may progress to cancer precursor lesions \cite{macios2022false,pinsky2015principles}. 


False discovery rate (FDR) control procedures are plausible techniques to keep the FP rate among the total positive classifications at a certain, user-defined level \cite{Benjamini1995Controlling}, when the risk of FP prediction is significantly higher than cost of FN predictions. The FDR control became popular with the emergence of high-throughput and shotgun experiments which measured large amount of distinct variables simultaneously per sample in a cost efficient way. For instance, microarrays measure the expression levels of thousands of genes simultaneously and FDR control allows to select promising genes for followup studies \cite{storey2002direct,storey2003statistical,barber2015controlling}. Tandem mass spectrometry can be used to quantify and identify thousands of proteins in a complex biological/chemical sample and FDR control allows to select the trustworthy protein identifications \cite{elias2007target}. Automatic patch clamp systems employ a deep-learning-based image processing module to automatically detect target cells (e.g. nerve cells in living brain) and then it calibrates the movement of a micropipette to approach the cell for future experiments \cite{koos2021automatic}. Cell morphological profiling in high-throughput imaging assays for studying compound libraries and human diseases \cite{moshkov2024learning} relies on automatic cell segmentation and detection often in 3D images \cite{falk2019u}. The FDR control here could curb the error in selecting wrong cells for subsequent experiments and feature quantification. 

The False Selection Rate (FRS) control procedure can restrain the FP and FN errors simultaneously \cite{zhao2023controlling}. This approach determines two thresholds, one to make definitive positive, and another one to make definitive negative predictions. Any samples not passing either thresholds are remain in {\em indecision}. Therefore, the misclassification error (accuracy) among definitive predictions can be bounded, while a subset of samples without predictions are returned for manual investigation by a human expert or for further data collection \cite{rava2021burden}. 

Typically, the FDR (and the FSR) control heavily relies on valid, i.e. uniformly distributed, p-values, while typical deep learning models produce raw discriminative scores, logits for samples. %Recent methods in outlier detection \cite{bates2023testing,marandon2023adaptive} and in classification \cite{rava2021burden, angelopoulos2021learn} have proposed using empirical p-values, also known as conformal p-values, in which a p-value of a given discriminative score $s$ is defined as the fraction of a set of validation  scores that are larger than or equal to $s$, formally: $p=(1+\sum_{s_i\in C} \mathds{1}\{s_i\ge s\})/(n+1)$. 
The empirical p-value (EPV), also known as conformal p-value, of a given discriminative score $s$ is defined as the fraction of a set of validation scores that are greater than or equal to $s$, formally: $p_s=(1+\sum_{s_i\in N_0} \mathone\{s_i \ge s\})/(n+1) \label{eq:epv}$ [eq. \ref{eq:epv}]. The set $N_0$ contains $n$ scores produced by a machine learning method (scoring function) on negative samples of the training set (or validation set), which can be constructed from the labeled training data or it can be created from the decoy peptides in tandem mass spectrometry data annotation \cite{elias2007target,danilova2019bias}. The EPVs of the negative test samples are super uniform under the null if the data distribution of the test and training samples are identically and independently distributed. This property is called {\em exchangeability}. In this case, the Benjamini-Hochberg (BH) procedure controls the FDR at $\pi_0\alpha$ level, where $\pi_0$ denotes the fraction of nulls (negatives) among the test samples, and $\alpha$ is a user-defined confidence level. The EPVs have been successfully used in recent methods for outlier detection \cite{bates2023testing,marandon2023adaptive} and in classification \cite{rava2021burden, angelopoulos2021learn} under the assumption of exchangeability. %  However, in practice, this assumption is often violated due to data shifts, batch effects, as discussed above; thus, the FDR control may be conservatively or liberally (anti-conservatively) biased. 

The performance of machine learning-based classifiers in software production phase is assumed to be the same as its performance on a validation set  \cite{dmlsbook2022}. Unfortunately, the  performance in real world applications can often be significantly worse than thought due to data distribution shift (a.k.a. covariate shift), data label distribution shift, and batch effects \cite{Candela2009DatasetShift} caused by some confounding factors. In life sciences, this shift can be attributed including, but not limited, to change in laboratory or environmental conditions, change in the reagents or atmospheric ozone levels, as well as to personnel differences and to variation of technical sources \cite{leek2010tackling}. Different high-throughput instruments produced by different manufacturers may also produce slightly differently distributed data about the same experiments. Quantitative morphological features of cells from multi-channel fluorescence microscopy images may depend on lamp intensities, filter patterns, dyes \cite{bray2016cell}. Tandem mass spectrometry data are affected by differences in experimental protocols and data acquisition conditions, reagent batches, or changes in instrumentation \cite{phua2022perspectives, vcuklina2021diagnostics}. These impacts may lead to incorrect conclusions about the experiments, and to improper clinical drug therapies, etc. Similarly in economics and industry, machine learning-based predictions may also become inaccurate after economic shock, technological advancement, geopolitical turbulence, and pandemic \cite{ramey2016macroeconomic}. For instance, energy consumption prediction needs to be re-calibrated after energy price change in order maintain carbon emission reasonably low \cite{clement2023coping}; credit risk assessment needs to be adapted to, for instance, monetary policy tightening, increased government expenditure, and income distribution shift caused by inflation \cite{kritzman2012regime,guo2023predict, Zhang:EECS-2021-262}.
% \cite{Zhang:EECS-2021-262}  % Inflation Reduction Act (IRA)



Statistically speaking, the data shift violates the exchangeability assumption resulting in conservatively or liberally (anti-conservatively) biased FDR control. 
Recent methods, which deal with data shift, aim to (1) detect data shift  \cite{ dasu2009change}, (2) identify features in tabular data, which cause data shift \cite{kulinski2020feature}, (3) detect and correct label shift \cite{lipton2018detecting}, (4) provide explainability about data shift \cite{budhathoki2021did,kulinski2023towards}, and aim to adapt to data shifts \cite{sui2024unleashing, zhang2021adaptive, zhang2022memo}. Most of the recent methods operate in the feature space and rely on data clustering or graphical causal models; and, are challenged by high-dimensional space, data sparsity (i.e. curse-of-dimensionality) \cite{donoho2000high}, and correlated features. 


In this paper we propose a test score calibration method in order to maintain accurate FDR control under data or class distribution shift. The prediction scores of the test samples are calibrated so that the approximated distribution of the prediction scores of the negative test samples (test null) aligns to the actual distribution of the prediction scores of the negative training samples (training null). Hence, we termed our method Test Null Adjustment (TNA). The empirical p-values of the adjusted test scores can be calculated with respect to the training null distribution with eq [1]. The test null adjusted empirical p-values (TNA EPVs)  can then be used to control the FDR with, for instance, with BH procedure. Valid p-values should be uniformly distributed over the range of $[0,1]$.  We propose to visually monitor the uniformity of the p-values with Q-Q plots, in which the p-values are plotted against their ordered normalized positions. Any deviation from the diagonal line of the Q-Q plot would indicate data shift.

% Domain shifts or batch effects can significantly degrade the performance of machine learning-based classifiers in software production without any warning signal. This can particularly be problematic in medical applications which involve human lives. The shift changes the shape of the prediction score distribution of the test samples compared to that of the training samples; hence, decision boundary re-calibration would be required to conform the expected performance standards. We present a simple, yet robust heuristic method to mitigate the bias in false discovery rate (FDR) control originating from data shift. Our method is based on an adjustment of the test prediction scores so that test null distribution (approximated distribution of the prediction scores of negative test samples) aligns to the training null distribution (actual distribution of prediction scores of the negative training samples), thereby we termed it Test Null Adjustment (TNA) method. Then, accurate empirical p-values of the adjusted test prediction scores can be calculated with respect to the training null distribution to be used in the Benjamini-Hochberg (BH) protocol for FDR control. The main advantage of the TNA method is that it's application does not require any domain specific knowledge, it works with multi-class scenario, it is fully data-driven, and it operates in the 1-dimensional space of the prediction scores ensuring stable numerical calculations. We have performed four experimental tests on biomedical image analysis domain to demonstrate that the TNA method is able to mitigate the bias in FDR induced by data shift. 


%In this paper, we propose a method to adapt the distribution of the scores of negative test samples to the null (i.e. the distribution of the scores of the negative training samples) in order to maintain accurate FDR control under data distribution shift. In detail, the empirical p-values of the test samples are calculated with using the whole negative training data as $H_0$ with eq. \ref{eq:epv}. In the absence of data shift, the p-values should be uniformly distributed. We propose to visually monitor the uniformity of the p-values with Q-Q plots, in which the p-values are plotted against their ordered normalized positions. Any deviation from the diagonal line of the Q-Q plot would indicate data shift. Alternatively, the Fisher's combinatorial test may also be used to test uniformity \cite{fisher1928statistical}. The distribution of the test scores are adjusted so that the mode of the predicted negative test samples fits to the distribution of scores of the negative training samples. \todo{akf-ab}{summarize the method and cite}. Then, the decision threshold can be calibrated with $\pi_0$-BH method directly for the actual test samples so that it controls the FDR at a predefined $\alpha$-level. This approach is also robust to data label shifts because the $pi_0$ estimation automatically considers the actual rate of the classes; therefore adaptation to altered class balances is not required.

The TNA method operates solely at the space of prediction score, which is typically one dimensional for each class. Therefore, TNA is not hindered by the curse-of-dimensionality \cite{donoho2000high}. It can be employed with any machine learning model, given that the classification is based on discriminative scores; this includes deep neural networks, black box models, support vector machines \cite{cristianini2000introduction}, with any kind of data such as tabular, text, audio, image, graph data. The TNA method is data driven; that is, it does not rely on any class of analytical distributions. The only assumption we have for the TNA method is that the score distributions resemble to the ones on Figure \ref{fig:binary}A.

%\begin{figure}
%	\centering
%	\includegraphics[width=5in]{img/synthetic_overview.pdf}
%	\caption{{\bf Error in decision making.} fd}
%	\label{fig:illustration}
%\end{figure}  


\section{Test null adjustment (TNA) method}
\subsection{Preliminaries}

Let us suppose that we are given a domain $D$ such as images, text, speech, graphs, time series, tabular data, or the mix of these; furthermore, let $X\subseteq D$ and $T\subseteq D$ denote the training and  the test data samples, respectively. Let $Y_t\in\{0,1\}$ denote the true class label of sample $t\in D$. Let $f:D\rightarrow R$ be an appropriately trained binary discriminator (classifier), such as a deep neural network, to predict positive instances, where $R$ denotes the real numbers. Without loss of generality, we assume that higher score $f(t)$ indicates a stronger membership to the positive class. The training null distribution is constructed as $X_0=\{f(t):Y_t = 0,\,\,  t\in X\}$ from the scores of negative instances, the training target distribution is constructed as $X_1=\{f(t): Y_t = 1,\,\,  t\in X\}$ from the scores of positive instances, and the training score distribution is $X_s=\{f(t): t\in X\}$. The true test null $T_0$, true test target $T_1$, and the test score $T_s$ distributions are constructed in a similar way with using true labels. We say that we trust (or accept) a  sample $t$ predicted as positive if and only if $f(t)>s_t$ for a given decision threshold  $s_t$ and we make indecision for samples if $f(t) < s_t$. The false discovery proportion (FDP) in a set of samples $K=\{t\in D\}$ at a score threshold $s_t$ is calculated as 
\begin{equation*}
	FDP(s_t,K)=\frac{1+|\{t\in K: Y_t=0,\,f(t)\ge s_t\}|}{|\{t\in K:f(t)\ge s_t\}|},
\end{equation*}
\noindent where $|.|$ denotes set cardinality. We reserve the FDR for the statistical expectation of FDP. FDR control methods aim to calibrate the threshold  $s_t$ so that the error rate among trusted predictions is at most level of $\alpha$. % We note that FDR control is related to FSR control in this case, even for $K=2$.%False positives are calculated as $FP=|\{t:Y_t\ne L(t)\}|$
%A label of a sample $t\in D$ is predicted $L(t)=sign(f(t))$. Let $N_0=\{f(t):t\in X,\, Y_t=0\}$,  the null distribution; that is, the set of the discriminative scores of the negative training samples.

% For binary classification problems (K=2), let $f:D\rightarrow R$ be a trained discriminator (classifier), $N_0=\{f(t):t\in X,\, Y_t=0\}$ the train null distribution,  $N_1=\{f(t):t\in X,\, Y_t=1\}$ the train target distribution, and $FDP(s_t)=(1+{|\{t: Y_t=0,\,f(t)\ge s_t\}|)/|\{t:f(t)\ge s_t\}|}$. We say that we trust the positive prediction of sample $t$ if and only if $f(t)>s_t$. We note that the FDR control is not related to FSR control in this case. 

The empirical  p-values (EPVs) of the test samples are calculated with using eq. \ref{eq:epv} with respect to $X_0$. The p-value calculation can be time consuming for large $X_0$ sets, in this case, a binned, empirical cumulative distribution function of $X_0$ could be used to calculate the p-values accordingly in a fast way. Then, the FDR in a set $T$ can be controlled with using the $BH$ protocol and the EPVs at a predetermined $\alpha$ level. For the sake of simplicity, we replace the $\alpha$ with $\alpha/\hat{\pi_0}$ inside BH algorithm, where $\hat{\pi_0}$ is an estimation of $\pi_0$ introduced by Storey \cite{storey2004strong} in the following way: $\hat{\pi_0} = (1+\sum_{i=1}^m \mathone\{p_i\ge\lambda\} )/(m(1-\lambda))$ for $\lambda\in (0,1)$. The q-value of a test sample $t\in T$ is defined as $q_t=\min_{z\le f(t)}FDP( z, T )$; that is the minimum $\alpha$ level when the sample $t$ would become a trusted positive prediction. %A test sample with smaller q-value indicates that it is a more trusted positive prediction. 
The smaller the q-value of a test sample $t$ is, the more trusted positive prediction $t$ is. Note that, the q-value of a sample not only depends on its prediction score, but it also depends on the other samples in the set. 
%
% THIS IS WITH MULTI CLASS CLASSIFICATION
%Let us suppose that we are given a domain $D$ such as images, text, speech, graphs, time series, tabular data, or the mix of these; furthermore, let $X\subseteq D$ and $T\subseteq D$ denote the training and  the test data samples, respectively. Let $Y_t\in\{0,\dots, K-1\}$ denote the true class label of sample $t\in D$. Let $f_k:D\rightarrow R$ be appropriately trained discriminators (classifiers), such as a deep neural network, to indicate class memberships ($k=0,1,\dots,K-1$), where $R$ denotes the real values. Without loss of generality, we assume that higher score $f_k(t)$ indicates a stronger membership to class $k$. Then, let $f(t)= \max_k\{f_k(t) \}$ and $L(t)=\text{argmax}_k\{f_k(t)\}$ be the predicted class label for $t\in D$. The train null distribution is constructed as $N_0=\{f(t): L(t) \ne Y_t,\,\,  t\in X\}$ from the scores of incorrect classifications, the train target distribution is constructed as $N_1=\{f(t): L(t) = Y_t,\,\,  t\in X\}$ from the scores of correct classifications. The true test null and true test target distributions are constructed in a similar way from the test samples with using the true labels, respectively.  We say that, we trust a prediction $L(t)$ for a sample $t$  if $f(t)\ge s_t$ for a decision threshold $s_t$, and we make indecision for samples if $f(t) < s_t$. The false discovery proportion (FDP) in a set of samples $\{t\in D\}$ at a score threshold $s_t$ is calculated as 
%\begin{equation}
%	FDP(s_t)=\frac{1+|\{t: Y_t\ne L(t),\,f(t)\ge s_t\}|}{|\{t:f(t)\ge s_t\}|},
%\end{equation}
%\noindent where $|.|$ denotes set cardinality. We reserve the FDR for the statistical expectation of FDP. FDR control aims to calibrate the $s_t$ so that the error rate among trusted predictions is at level of $\alpha$. We note that FDR control is related to FSR control in this case, even for $K=2$.%False positives are calculated as $FP=|\{t:Y_t\ne L(t)\}|$
%%A label of a sample $t\in D$ is predicted $L(t)=sign(f(t))$. Let $N_0=\{f(t):t\in X,\, Y_t=0\}$,  the null distribution; that is, the set of the discriminative scores of the negative training samples.
%
%For binary classification problems (K=2), let $f:D\rightarrow R$ be a trained discriminator (classifier), $N_0=\{f(t):t\in X,\, Y_t=0\}$ the train null distribution,  $N_1=\{f(t):t\in X,\, Y_t=1\}$ the train target distribution, and $FDP(s_t)=(1+{|\{t: Y_t=0,\,f(t)\ge s_t\}|)/|\{t:f(t)\ge s_t\}|}$. We say that we trust the positive prediction of sample $t$ if and only if $f(t)>s_t$. We note that the FDR control is not related to FSR control in this case. 
%
%The p-values of the test samples are calculated with using eq. \ref{eq:epv} with respect to $N_0$. The p-value calculation can be time consuming for large $N_0$ sets, in this case, a binned, empirical cumulative distribution function of $N_0$ could be used to calculate the p-values accordingly. The FDR in a set $T$ can be controlled with using the $BH$ protocol at a predetermined $\alpha$ level. For the sake of simplicity, we replace the $\alpha$ with $\alpha/\hat{\pi_0}$ inside BH algorithm, where $\hat{\pi_0}$ is an estimation of $\pi_0$ introduced by Storey \cite{storey2004strong}: $\hat{\pi_0} = (1+\sum_{i=1}^m \mathone\{p_i\ge\lambda\} )/(m(1-\lambda))$ for $\lambda\in (0,1)$. The q-value of a test sample $t\in T$ is defined as $q_t=\min_{z:\le f(t)}FDP( z )$, that is the minimum $\alpha$ level when the sample $t$ would become a trusted prediction. The q-value of a sample would depend on the other samples in the set. The lower the q-value the most trusted the prediction is.

%\paragraph{Example 1.} For demonstration, we trained a two-layer convolutional neural network containing 8 kernels with size of $3\times3$ at each layers, respectively, to predict the hand-written digits of the MNIST samples. The MNIST dataset contains 60K training and 10K test images of 28x28 pixels in 10 classes. There is no shift in the test set. The accuracy of the trained model is 0.975. The score distributions 

\paragraph{LTT.} The LTT .. \todo{AB}{Add a description how you run LTT. how it was parameteriezed, etc.}

\paragraph{Example 	 1.} \label{ex:vanilla} We used the well-known MNIST dataset for demonstration. The MNIST dataset contains 60K training and 10K test images of 28x28 grey-scaled pixels in 10 classes. There is no shift between the training and test data. We trained a convolutional neural network to classify the digit '1' as target (positive) against all the other digits (negative) (binary classification for the sake of simplicity). Our ConvNet model contained two convolutional layers consisting of 8 kernels with size of $3\times3$ at each layers, and it was trained with Adam optimizer with a learning rate of 0.01 for three epochs. Our trained model achieved a 99\% accuracy on the test set. 

The relevant results are presented in Figure \ref{fig:binary}. The Figure \ref{fig:binary}A shows the training and test null and target distributions. %This plot indicates that there is no shift between the training and test distributions. 
The Figure \ref{fig:binary}B shows a QQ plot of the EPVs against the theoretical uniform distribution; that is the scatter plot of the EPV vs. its normalized rank in our case. The dots line up along the diagonal line which shows that the EPVs calculated are uniformly distributed and valid. This also indicates that there is no shift between the training and test distributions. %We also performed the Fisher's combined test to check whether the empirical p-values are uniformly distribution. The test produces a p-value of 0.934 indicating uniformly distributed empirical p-values. 
The Figure \ref{fig:binary}C shows the number of accepted (trusted) predictions as positives at various q-values ($\alpha$ level of FDR). The ground truth (black solid line) was calculated using the true test labels and we consider it correct. Any FDR controlling method (a) yielding more trusted predictions than the ground truth is liberally biased, and (b) yielding less trusted predictions is conservatively biased. The BH procedure with EPVs (red dashed line) yields closely the same number of trusted prediction at various FDR levels of $\alpha$ (calculated without using the true test labels). This indicates that the EVPs are uniform and BH procedure is an unbiased method. The LTT method also accurately controls the FDR at any $\alpha$ levels (blue dotted line) in the absence of data or class distribution shift. Finally, the Figure \ref{fig:binary}D shows that the deviation of the estimated FDRs from the true FDRs is small.


%, and with the BH procedure using the standard EPVs (red dashed lines)
%Therefore, BH can control the FDR accurately at various levels (q-values). This is shown in \ref{fig:binary}C, where the true (black solid line) and the accepted predictions (green dashed line) are closely overlap at various FDR levels (q-values). As a comparison, we also evaluated the LTT method \cite{angelopoulos2021learn} (dashed blue line), and results confirm that LTT is also an accurate method to control the FDR in the absence of data distribution shift. 

\todo{AB}{Remove the TNA from figure 2B and 2C}.

\begin{figure}
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=1.78in]{img/cls_overview.png}&
		\includegraphics[width=1.73in]{img/cnn_QQ_classical.png} &
		\includegraphics[width=1.69in]{img/cnn_classical_fdr_control.png} &
		\includegraphics[width=1.68in]{img/cnn_FDRQQ_classical.png} \\
		A & B & C & D \\
	\end{tabular}
	\caption{{\bf FDR control in MNIST with EPVs. } (A) The discriminative score distributions. \todo{AB}{The legends should be: Train null ($X_0$), Train target ($X_1$), Test null ($T_0$), Test target ($T_1$).} (B) The QQ plot of the EPVs of the test samples against the theoretical uniform distribution. (C) The number of the accepted (trusted) positive predictions at various q-values calculated with true labels (black solid line), BH with EPVs (red dashed line), and LTT (blue dotted line). (D) Deviation between the the true and the estimated FDR produced by BH with EPV (red line) and LTT (blue dots), respectively.}
	\label{fig:binary}
\end{figure}
 
\paragraph{TNA protocol.} When the training null distribution $X_0$ is different from the test null distribution $T_0$, then the EPVs will not be uniformly distributed and the BH procedure results in liberal or conservative bias in FDR estimation. In the following steps, we describe the TNA method that adjusts the prediction scores so that the approximated test null distribution $\hat{T}_0$ aligns to the actual training null distribution $X_0$. Our method relies on the following formulation: $X_s = (\pi_0) \cdot X_0 + (1-\pi_0) \cdot X_1$, where $\pi_0$ is the proportion of the negative training samples; therefore, the null distribution can be expressed as $X_0 = (X_s - (1-\pi_0) \cdot X_1)/\pi_0$.

\begin{enumerate}%[label=\arabic*.]
	\itemsep-3pt  		
	\item Get the density histograms of $X_0$, $X_1$, and $T_s$ distributions with identical bin borders and denote them as $\overline{X}_0$, $\overline{X}_1$, and $\overline{T}_s$, respectively. The $\overline{X}[i]$ denotes the value of bin $i$.  We used 100 bins in our experiments and it worked fine.
	
	\item Let $\hat{\pi}_0$ be an approximated proportions of the negative test predictions calculated with using the number of positive and negative test predictions. 
	
	\item We create a new density histogram $\hat{T}_0$ that will be an approximation of the test null $T_0$. The value of the histogram bin $i$ is calculated  by $\hat{T}_0[i] = (\overline{T}_s[i] -  (1-\hat{\pi}_0)\hat{T_1}[i])/\hat{\pi}_0$. The unknown $T_1$ is approximated by $T_s$ scaled with the proportion of the target calculated from the training data; that is, 
	$$\hat{T_1}[i] = \overline{T_s}[i]\cdot \frac {\overline{X_1}[i]}{\hat{\pi}_0 \overline{X_0}[i] + (1-\hat{\pi}_0) \overline{X_1}[i]}.$$ 
	Mind that the training null and target distributions ($X_0,X_1$) are weighted with test class proportions  $\hat{\pi}_0$. The outcome of division by zero is set to zero.
		
	\item Let $\mu_{X_0}$, $\sigma_{X_0}$, $\mu_{\hat{T_0}}$, and $\sigma_{\hat{T_0}}$ are the mean and std of the $X_0$ and the $\hat{T_0}$ distributions, respectively. 
	
	\item Adjust the test scores $\hat{t_s} = (t_s-\mu_{\hat{T_0}})/\sigma_{\hat{T_0}} \cdot \sigma_{\hat{X_0}} + \mu_{X_0}$.
\end{enumerate}

	Finally, calculate the TNA EPVs for each $\hat{t_s}$ with respect to the $X_0$ distribution, estimate  $\hat{\pi_0}$ for the test instances with the Storey method with $\lambda= 0.8$, and run BH procedure to control the FDR at a desired level. 
	
	We note that $T_1$ could be approximated simply with $X_1$; however, this approximation would be more off when $T_1$ and $X_1$ differ because the difference $T_1-X_1$ would be accounted for the null distribution.
	
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=3in]{img/TNA_sketch.jpg}
%	\caption{{\bf Illustration of the TNA method.} d}
%	\label{fig:tna-skecth}
%\end{figure}  
	
	
%	
%	1. train 0, as fixed, named N0. calculate mu and sigma for N0. 
%	2. take negative test predictions. calculate mu and sigma for test negative.
%	3. move all test negative to train 0. This is not critical. 
%	4. what do you do with test positive? can we also move test postivie with 
%    4a . I think, should be done: test_instance_score: t_i := (t_i-mu)/sigma. skip this.
%% Andrey says, steps 1-4 are not important.
%    5. histograming train scores and test scores with identical bin (borders).
%    6. calculate proportion of train negatives among train scores in each bin.
%    7. estimate pi_0, based on the number of test positive and test negataive predictions. 
%    8. bins on positive side of the x-axis: 
%    9. here you do some scaling. 
%    10. bin i. 
%    11. calculate again mu and sigma of new test negative,  
%    12. adjust test negative to N0,
%    13. calculate p-values with the adjusted test negative? Use Storey method to estimate lambda, and estimate the pi_0.
%    14. use BH to control FDR. 
%	
	
%	
%	\item The train null distribution $N_0$ is adjusted to the distribution of the negative test predictions. Let $\mu_{T_0}$ and $\sigma_{T_0}$ denote the mean and the standard deviation of $T_0$. The train null is adjusted via $\hat{N_0} = \frac{N_0-\nu_{T_0}}{\sigma_{T_0}+\epsilon}$, where $\epsilon$ is a small numerical regularization parameter.
%	
%	\item The proportions of the nulls among the test instances is estimated $\hat{\pi_0}=N_t/(N_t+P_t)$, where $N$ and $P$ denote the  number of negative and positive test predictions, respectively. We note that, this ratio can be very different from the one of the train data. 
%	
%	\item A histograms of $\hat{N_0}$ and $T$ is constructed with the same binning. The histograms are denoted with $H_{N_0}$ and $H_T$, and $H[i]$ denotes the counts in bin $i$. In our experiments the bin width was set to 1.0 and it worked fine.
%	
%	\item Finally, we construct the histogram of the test null $H_{T_0}$ via $H_{T_0}[i]= H_T[i]*c_i*\hat{\pi_0}$, where $c_i=P[i]/N[i]$, i.e. the fraction of the negatives among the train data in the ith bin, in the histogram.


\begin{figure}[h!]
	\centering
	\begin{tabular}{cccc}
		\multicolumn{4}{l}{\bf (i) Data distribution shift:}\\
		\includegraphics[width=1.66in]{img/cnn_QQ_intensity_down.png}&
		\includegraphics[width=1.72in]{img/cnn_intensity_down_fdr_control_loc.png} &
		\includegraphics[width=1.7in]{img/cnn_intensity_down_fdr_control.png} & 
		\includegraphics[width=1.59in]{img/cnn_FDPscat_intensity_down.png} \\
		\multicolumn{4}{l}{\bf (ii) Class distribution shit:}\\
		\includegraphics[width=1.66in]{img/cnn_QQ_balanced.png}&
		\includegraphics[width=1.72in]{img/cnn_balanced_fdr_control_loc.png} &
		\includegraphics[width=1.7in]{img/cnn_balanced_fdr_control.png} & 
		\includegraphics[width=1.59in]{img/cnn_FDPscat_balanced.png} \\		
		A & B & C & D \\
	\end{tabular}
	\caption{{\bf  FDR control with empirical p-values.}
		(A) QQ plot of the EPVs (red dots) and the EPVs with TNA (green dots) against the theoretical uniform distribution. The results of the Fisher's combination tests are indicated in the legend. (B) The number of accepted classifications as a function of the Q-values obtained with (i) ground truth (black line), (ii) BH with EPV (red), (iii) BH with EPV with TNA (green), and (iv) LTT (blue). (C) Same as (B) with over the entire Q-value range. (D) Deviation of the estimated FDR from the actual FDR obtained with (i) BH with EPV (red line), (ii) BH with EPV and TNA (green line, (iii) LTT (blue line).}
	\label{fig:mnist_shfit}
\end{figure}

	
\paragraph{Example 2.} To illustrate the case of data distribution shift, we down-scaled the pixel intensity by 10\% of the test images in the MNIST dataset (but training images remained the same). The accuracy of the CNN classifier (the same as trained in Example \ref{ex:vanilla}) remained at 99 \%. The evaluation plots are shown in Figure \ref{fig:mnist_shfit}-(i). The plots and tests show that standard EPVs became biased in the case of data distribution shift; however, TNA can correct the EPVs so that FDR control becomes unbiased.


\paragraph{Example 3.} To illustrate the case of class distribution shift, we resampled the test data so that number of true negative and positive instances are equal. This approach was used in other articles, e.g.  \cite{joseph_d__viviano__2019}. The accuracy of the CNN classifier (the same as trained in Example \ref{ex:vanilla}) remaind at 99 \%. The evaluation plots are shown in Figure \ref{fig:mnist_shfit}-(ii). The results show that, in general, the EPV methods can adjust to the changed class proportions thanks to the $\pi_0$ estimation among the test instances for the BH protocol. However, the LTT method calibrates its parameters about class proportion with using a validation set, hence it becomes biased when class proportion change in the test data. 
%
%\begin{figure}[h!]
%	\centering
%	\begin{tabular}{cccc}
%		\includegraphics[width=1.7in]{img/cnn_QQ_balanced.png}&
%		\includegraphics[width=1.7in]{img/cnn_balanced_fdr_control_loc.png} &
%		\includegraphics[width=1.7in]{img/cnn_balanced_fdr_control.png} & 
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_balanced.png} \\		
%		A & B & C & D \\
%	\end{tabular}
%	\caption{{\bf  FDR control with EPV under altered class distribution shit.}
%		(A) QQ plot of the EPVs (red dots) and the EPVs with TNA (green dots) against the theoretical uniform distribution. The results of the Fisher's combination tests are indicated in the legend. (B) The number of accepted classifications as a function of the Q-values obtained with (i) ground truth (black line), (ii) BH with EPV (red), (iii) BH with EPV with TNA (green), and (iv) LTT (blue). (C) Same as (B) with over the entire Q-value range. (D) Deviation of the estimated FDR from the actual FDR obtained with (i) BH with EPV (red line), (ii) BH with EPV and TNA (green line), (iii) LTT (blue line).}
%	\label{fig:mnist_rebalance}
%\end{figure}

\section{Experimental results}

\subsection{PCam: tumor classification}



The PatchCamelyon (PCam) benchmark dataset is a collection of histopathologic scans of lymph node sections \cite{Veeling2018-qh} derived from the Camelyon 16 challenge \cite{camelyon16}. PCam consists of 327,680 colored images of size 96x96 each. The challenge is to identify the presence (positive) or absence (negative) of any histopathology in one image. The dataset creators of PCam underline that all the positive and negative instances in the training, validation, test sets are equally distributed, resulting in a class balance of 1:1. We used the pre-trained resnet34-pcam model, which is a popular, deep residual network from the TIA (Tissue Image Analysis) toolbox \cite{Pocock2022} with an overall f1-score of 0.889, as stated by the authors. We evaluated the uniformity of the p-values of the test samples and the performance of the FDR controlling methods. The results are shown in the first row of Figure \ref{fig:I}.  The QQ plots (Fig. \ref{fig:I}-(i)-A) of the test EPVs (red dots) and the test NTA EPVs (green dots) indicate that the test p-values are slightly biased, that might indicate a slight overfitting and/or distribution shift between the training and test datasets. This is in accordance with the results of the Fisher Combinatorial Tests. \todo{AKF}{Discuss the FCT results.} The plots on Fig. \ref{fig:I}-(i)-B-D indicate that all three FDR control methods are fairly accurate with this dataset especially at critical $\alpha$ levels (0-0.1), despite the small bias in the EPVs. The NTA method manages to reduce the bias from EPVs and yields more accurate FDR control with BH (green line), than the BH mehtod with the original EPVs (red line) in general; however, LTT outperformed both BH-based methods in this benchmark. 

In our opinion, the actual class proportions in real-life applications can be significantly different from 1:1, making the FDR control with methods like LTT inaccurate in practical applications. We manually resampled the test data so that  positive and negative class balance became 31\%:69\%, following the original class balance of the Camelyon 16 dataset. The evaluation plots are shown in the second row of Figure \ref{fig:I}. The QQ plot (Fig. \ref{fig:I}-(ii)-A) reveals that the EPVs (red dots) become slightly biased; however, our NTA method manages to reduce this bias from the EPVs (green dots). \todo{AKF}{Discuss the FCT results.}  The LTT method became liberal in FDR control, the BH protocol with standard EPVs resulted in conservative FDR control; whereas, the BH with the TNA EPVs remained accurate, especially at critical $\alpha$ levels (0-0.1) (Fig. \ref{fig:I}-(ii)-C).

%However, the situation changes when we resampled the data 
%
%; while the positive and negative class balance in the Camelyon 16 test dataset is 31\% :69\%. Therefore,  the actual true class proportions in PCam might be significantly different in real-life applications, making the FDR control with methods like LTT inaccurate in practical applications. We manually resampled the test data sets of the PCam so that the class ratios become the same as of the Camelyon 16 dataset, because,  in our opinion,  this class ratio is probably more realistic for real life scenarios. We used the pretrained resnet34-pcam model, which is a popular, deep residual network from the TIA (Tissue Image Analysis) toolbox \cite{Pocock2022} with an overall f1-score of 0.889, as stated by the authors.
%
%
%The evaluation of FDR controlling methods on the PCam dataset is shown in the Figure \ref{fig:pcam} with original class distribution. The plots indicate that all three FDR control methods are fairly accurate in practice. Perhaps, the QQ plots of the test EPVs (red dots) and the test NTA EPVs (green dots) in the Figure \ref{fig:pcam}A indicate that the test p-values are slightly biased; however, the NSA method manages to reduce this bias in the EPVs. This indicates a slight overfitting and/or distribution shifts between the training and test data sets. However, when we changed the even class distribution to 31\%:69\%, the LTT FDR control method resulted in liberal FDR control, the BH protocol with standard EPVs resulted in conservative FDR control; whereas, the BH with the TNA EPVs resulted in accurate FDR control,  especially at critical $\alpha$ levels (0-0.1). These results are shown in the Figure \ref{fig:pcam_rebalance}.


\begin{figure}[h!]
	\centering
	\begin{tabular}{cccc}
 		\includegraphics[width=1.in]{img/pcam1.jpg} &
		\includegraphics[width=1.in]{img/pcam2.jpg} & 
            \includegraphics[width=1.in]{img/pcam3.jpg} &
             \includegraphics[width=1.in]{img/pcam4.jpg}
        \end{tabular}
	\caption{{\bf Instances of PCam dataset.}}
	\label{fig:pcam_example}
\end{figure} 




\begin{figure}[h!]
	
	\begin{tabular}{cccc}
		\multicolumn{4}{l}{\bf (i) Standard PCam dataset:}\\		
		\includegraphics[width=1.7in]{img/cnn_QQ_pcam.pdf} &
		\includegraphics[width=1.7in]{img/cnn_pcam_fdr_control.pdf} & 
		\includegraphics[width=1.7in]{img/cnn_pcam_fdr_control_loc.pdf} &
		\includegraphics[width=1.7in]{img/cnn_FDPscat_pcam.pdf}\\	
		\multicolumn{4}{l}{\bf (ii) PCam with class distribution shift:}\\		
		\includegraphics[width=1.7in]{img/cnn_QQ_pcam_balanced.png} &
		\includegraphics[width=1.7in]{img/cnn_pcam_balanced_fdr_control.png} & 
		\includegraphics[width=1.7in]{img/cnn_pcam_balanced_fdr_control_loc.png} &
		\includegraphics[width=1.7in]{img/cnn_FDPscat_pcam_balanced.png}\\
		\multicolumn{4}{l}{\bf (iii) Standard CheXpert dataset:}\\	
		\includegraphics[width=1.7in]{img/cnn_QQ_chx.png} 		&
		\includegraphics[width=1.7in]{img/cnn_chx_fdr_control.png} & 
		\includegraphics[width=1.7in]{img/cnn_chx_fdr_control_loc.png} & 
		\includegraphics[width=1.7in]{img/cnn_FDPscat_chx.png}\\
		\multicolumn{4}{l}{\bf (iv) CheXpert dataset with class distribution shift:}\\	
		\includegraphics[width=1.7in]{img/cnn_QQ_chx_balanced.png} 		&
		\includegraphics[width=1.7in]{img/cnn_chx_balanced_fdr_control.png} & 
		\includegraphics[width=1.7in]{img/cnn_chx_balanced_fdr_control_loc.png} & 
		\includegraphics[width=1.7in]{img/cnn_FDPscat_chx_balanced.png}\\
		A & B & C & D
	\end{tabular}
	\caption{{\bf Performance evaluation of FDR controlling methods.} (A) QQ plot of the EPVs (red dots) and the TNA EPVs (green dots) against the theoretical uniform distribution. The results of the Fisher's combination tests are indicated in the legend. (B) The number of accepted classifications as a function of the Q-values obtained with (i) ground truth (black line), (ii) BH with EPV (red), (iii) BH with TNA EPV (green), and (iv) LTT (blue). (C) Same as (B) but over a critical Q-value range. (D) Deviation of the estimated FDR from the actual FDR obtained with (i) BH with EPV (red line), (ii) BH with EPV and TNA (green line), (iii) LTT (blue line).}
	\label{fig:I}
\end{figure} 

\begin{figure}[h!]
	\centering
	\includegraphics[width=5.in]{img/chx.png}
	\caption{{\bf Instances of CheXpert dataset.}}
	\label{fig:pcam_example}
\end{figure} 

\subsection{CheXpert: chest x-ray dataset}

The CheXpert dataset contains a total of 224,316 radiographs of both frontal and lateral views of the thorax from $\sim$65,000 patients in order to facilitate the automated interpretation of chest X-rays for multi-label classification tasks (i.e. multiple labels can be predicted for each instances) with a total of 14 labels. We used the deep neural network model, called DenseNet121, from the \texttt{Torchxrayvision} package \cite{cohen2020limits,Cohen2022xrv}, specifically developed for this CheXpert dataset. For the sake of simplicity, we turned CheXpert dataset into a binary classification task by treating all instances of pleural effusion as positive and all the others as negative. We selected this label because this was the most abundant disease in the dataset. The proportion of the positive class became 27 \% and 23 \% in the training and test datasets, respectively and the DenseNet121 model achieved accuracies of 89 \% and 84 \% on the training and test sets, respectively.

We evaluated the three FDR controlling methods and plotted the results on Figure \ref{fig:I}-(iii). Interestingly, the most confident prediction of the DenseNet121 model in the test set is wrong. This can be seen by the flat region of the ground truth evaluation (black line) at low q-values around 0-0.05 (Fig, \ref{fig:I}-(iii)-C). Luckily, all three FDR controlling methods managed to capture this and remain close to the ground truth (Fig, \ref{fig:I}-(iii)-B-D). Both EPVs, standard (red dots) and NTA EPVs (green dots) tend to be off at the critical range (p-value $<$ 0.1), but this does not affect the FDR controlling results in this benchmark, perhaps because of the confident incorrect predictions. Overall, we conclude that all three methods provide accurate FDR control. In our opinion, this indicates the absence of any database distribution shits. The also shows that out TNA method does not mess up with the training or test null distributions when there is no data shift.

The actual frequency of pleural effusion may be different among all chest x-ray radiograph performed and it may vary over locations, age groups, in-patients and out-patients \cite{Zaki2024,Cashen2017PleuralEA}. In the united states, 275 million conventional radiology procedures are performed annually \cite{mahesh2022patient}, but only around 1.5 million of US citizens develop pleural effusion \cite{Cashen2017PleuralEA}. Moreover, pleural effusion develops in about 20-40\% of in-patient with pneumonia \cite{shebl2018parapneumonic}. Whatever the real abundance of the pleural effusion is among  chest x-rays performed, it can be significantly different from the 27 \% of the CheXpert dataset. Taking this into account, we re-sampled the test set in the CheXpert dataset to achieve a lower positive class ratio of 13\%, and the results of the re-evaluation is shown in Figure \ref{fig:I}-(iv). The EVPs and the NTA EPVs are both slightly biased by the same amount, but both passed the FTC test with a p-values of around 1.0. The BH with the standard EVPs achives the most accurate FDR control over the entire range of $\alpha$ levels (0-1) in general; however, BH with NTA EPVs provide the most accurate FDR control over the critical region of  $\alpha$ levels (0-0.1). LTT method is very sensitive to class distribution shift and in this case too it become liberally biased FDR controlling method.


%On the other hand, when diving into the real-world frequency of pleural effusion diagnosis asserted, the percentages fall down drastically. For instance, around 1.5 million of US citizens face pleural effusion annually, making only 0.005\% of the entire population. Similar statistics are listed for each developed country, with even lower numbers being presented \cite{Zaki2024}\cite{Cashen2017PleuralEA}. Hence, it can be confidently underscored that dataset's share of pleural effusion patients is only a single distribution, being absolutely erratic to major class disitrbution alterations. Thefore, we introduce the second experiment with CheXpert dataset, where proportion of postiive class is reduced to only 13\%, a two-times fall in general.  

% This comprehensive corpus encompasses both frontal and lateral views of the thorax. The principal objective of this dataset is to facilitate the automated interpretation of chest X-rays for the task of multi-label classification. It is noteworthy that the dataset is enhanced with uncertainty labels, which provide additional context surrounding the confidence levels of the predictions made by automated systems. Furthermore, the CheXpert dataset includes radiologist-labeled reference standard evaluation sets, which serve as a benchmark for assessing the accuracy and reliability of automated interpretation algorithms in comparison to human expert evaluations. The integration of diverse data types and reference standards positions the CheXpert dataset as a significant resource for advancing research and development in medical imaging and machine learning applications.

%As for the selected model, a DenseNet121, being specifically elaborated for the CheXpert dataset as a part of extensive package called 'Torchxrayvision' \cite{cohen2020limits}\cite{Cohen2022xrv}, was chosen. While it initially was designed to solve a task of multilabel classification among xray images, we further finetuned it for classifying 'Pleural Effusion' only. The proportion of positive class within the training set reached 27\%, insignificantly overwhelming the same number for test - 23\%. Similar situation is seen in context of the measured quality: 89\% accuracy against 84\% for train and test respectively. Therefore, no significant shift of data or class proportion rebalance is identified, which in turn might lead to challenges for those models trained when dealing with real-life applications. For instance, in \cite{joseph_d__viviano__2019} authors elaborate on the issue of poor generalisation abilities of algorithms being trained on a specific dataset of xray images and then tested on another, being a straight consequence of data distribution shifts.



\subsection{TissueNet: cells segmentation}

\begin{figure}
    \centering
	\includegraphics[width=5in]{img/tissuenet.pdf}
	\caption{{\bf Instance of TissueNet dataset.} Two-channel original images are being processed by the model, in turn producing masks \& flows to correctly segment cells}
	\label{fig:tissue_example}
\end{figure} 

TissueNet dataset contains 1.3 million microscopic images of cells obtained from six various platforms and nine organs including both histologically healthy and diseased tissues. Each images has a size of 512x512 pixels. Each image was manually segmented at pixel level and paired whole-cell and nuclear annotations. Our binary classification task was to predict whether a pixel is inside a cell (positive) or not (negative). The overall positive:negative ration was 59:41\% in the training data and 66:34\% in the test data; however, the class ratio varies over the images considerably. We used the Cellpose pre-trained deep neural network model \cite{cellpose}, we fine tuned it for five epochs, and it achieved 95\% and 94\% of accuracies  on training and test data, respectively. Our evaluation is reported on Figure \ref{fig:ii}-(i). The plots reveal that the BH protocol with standard EPVs becomes liberally biased (Fig. \ref{fig:ii}-(i)-B-C) possibly originating from data distribution shift or from overfitting. Both, LTT and the BH with TNA EPV remain accurate in general. 

The situation changes when we evaluate the segmentation problem only with one image. We arbitrarily selected one image (ID: \todo{AB}{image id} and run the Cellpose model to classify each pixels. The positive and negative class balance was: \todo{ab}{add the ratio}. The QQ plot (Fig. \ref{fig:ii}-(ii)-A) of the test EPVs (red dots) indicates that the scores of negative samples from training and test datasets are different; however the FCT suggests that they are uniformly distributed, oddly. The NTA EPVs on the QQ plot shows that the TNA method has managed to adjust the p-values of the test samples a bit close to the uniform distribution at the critical range (p-values 0-0.5) but higher p-values are deviate the uniform distribution by a large magnitude, even the FCT rejects the hypothesis of the p-values being uniformly distributed.  The plots on Fig. \ref{fig:ii}-(ii)-B-D indicate that LTT become liberally biased, the BH method with standard EPVs become strongly conservative. The BH with TNA EPVs remains accurate in the critical region of $\alpha$ leveles (0-0.4). Thus, the NTA method managed to correct the p-values at the critical range at least. 

\begin{figure}[h!]
	
	\begin{tabular}{cccc}
		\multicolumn{4}{l}{\bf (i) Standard TissueNet dataset:}\\		
		\includegraphics[width=1.7in]{img/cnn_QQ_cells_segment.png} &
		\includegraphics[width=1.7in]{img/cnn_cells_segment_fdr_control.png} & 
		\includegraphics[width=1.7in]{img/cnn_cells_segment_fdr_control_loc.png} & 
		\includegraphics[width=1.7in]{img/cnn_FDPscat_cells_segment.png}\\	
		\multicolumn{4}{l}{\bf (ii) TissueNet with single image:}\\		
		\includegraphics[width=1.7in]{img/cnn_QQ_cells_balanced.png} &
		\includegraphics[width=1.7in]{img/cnn_cells_balanced_fdr_control.png} & 
		\includegraphics[width=1.7in]{img/cnn_cells_balanced_fdr_control_loc.png} & 
		\includegraphics[width=1.7in]{img/cnn_FDPscat_cells_balanced.png}\\			
		\multicolumn{4}{l}{\bf (iii) Standard BCSS dataset:}\\		
		\includegraphics[width=1.7in]{img/cnn_QQ_multi_sa_bcss.png} &
		\includegraphics[width=2in]{img/cnn_multi_sa_bcss_fdr_control.png} &
		\includegraphics[width=2in]{img/cnn_multi_sa_bcss_fdr_control_loc.png}  &
		\includegraphics[width=1.7in]{img/cnn_FDPscat_sa_bcss.png}\\	
		A & B & C & D
	\end{tabular}
	\caption{{\bf Performance evaluation of FDR controlling methods.} Plot explanation is same as in Figure \ref{fig:pcam}. }
	\label{fig:ii}
\end{figure} 




\subsection{BCSS: Breast cancer semantic segmentation dataset}

\begin{figure}[h]
	\centering
	\includegraphics[width=3in]{img/bcss_instance.jpeg}
	\caption{{\bf An instance from BCSS dataset.}}
	\label{fig:bcss_example}
\end{figure} 

The Breast Cancer Semantic Segmentation (BCSS) dataset \cite{Amgad2019StructuredCE} contains around 20,000 image segmentation images annotated with Digital Slide Archive, with a particular focus on tissue regions derived from breast cancer imagery obtained from The Cancer Genome Atlas (TCGA). The image pixels are labeled by: 'Stroma', 'Tumor' and 'Others', and the corresponding class ratios in both the training and test sets are: 54\% : 38\% : 8\%. We used the deep learning model called \texttt{'fcn resnet50 unet-bcss'} from the TIA (Tissue Image Analysis) toolbox \cite{Pocock2022} to classify the pixels to one of the three classes. The model achieved an accuracies of 84 \% and 82 \% on the training and test datasets. 

We note that, because this dataset involves multi-class classification problem, we slightly modified the FDR control procedure. We calculated the p-values, the $\pi_0$ estimations and then the corresponding q-values with BH procedure with respect to single classes; then we associated the pixel class label to smallest q-value. Therefore, a trusted prediction means that a sample is correctly classified to its class. We note that, we did not evaluate LTT on this benchmark, because it is not clear how to adjust it to multi-class classification problems.

The evaluation of the FDR controlling methods are presented in Figure \ref{fig:ii}-(iii). At first glance, the results suggests that this datasets is challengeing and it  has a strong shift between the training and test data or the deep learning model has seriously overfitting. The QQ plots of the EPVs and the NTA EPVs are far from uniform (Fig. \ref{fig:ii}-(iii)-A). Over the critical $\alpha$ levels (0-0.1), the BH with standard EPV is conservatively biased, but the BH with TNA EVS are relatively accurate (Fig. \ref{fig:ii}-(iii)-C); however, over the broader $\alpha$ levels (0.1-0.4), BH seems to be more accurate with EVPs than with TNA EPVs. Our conclusion with this benchmark datasets is that the deviations in QQ plots and sharp turns in the plot of trusted predictions (without the ground truth) should be alarming to researchers about the performance of their models. 

%
%
%<<<<<<< HEAD
%=======
%\begin{figure}[h!]
%	
%	\begin{tabular}{cccc}
%		\multicolumn{4}{l}{\bf (i) Standard PCam dataset:}\\		
%		\includegraphics[width=1.7in]{img/cnn_QQ_pcam.pdf} &
%		\includegraphics[width=1.7in]{img/cnn_pcam_fdr_control.pdf} & 
%		\includegraphics[width=1.7in]{img/cnn_pcam_fdr_control_loc.pdf} &
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_pcam.pdf}\\	
%		\multicolumn{4}{l}{\bf (ii) PCam with class distribution shift.:}\\		
%		\includegraphics[width=1.7in]{img/cnn_QQ_pcam_balanced.png} &
%		\includegraphics[width=1.7in]{img/cnn_pcam_balanced_fdr_control.png} & 
%		\includegraphics[width=1.7in]{img/cnn_pcam_balanced_fdr_control_loc.png} &
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_pcam_balanced.png}\\
%		\multicolumn{4}{l}{\bf (iii) Standard TissueNet dataset:}\\		
%		\includegraphics[width=1.7in]{img/cnn_QQ_cells_segment.png} &
%		\includegraphics[width=1.7in]{img/cnn_cells_segment_fdr_control.png} & 
%		\includegraphics[width=1.7in]{img/cnn_cells_segment_fdr_control_loc.png} & 
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_cells_segment.png}\\	
%		\multicolumn{4}{l}{\bf (iv) TissueNet with class distribution shift.:}\\		
%		\includegraphics[width=1.7in]{img/cnn_QQ_cells_balanced.png} &
%		\includegraphics[width=1.7in]{img/cnn_cells_balanced_fdr_control.png} & 
%		\includegraphics[width=1.7in]{img/cnn_cells_balanced_fdr_control_loc.png} & 
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_cells_balanced.png}\\			
%		A & B & C & D
%	\end{tabular}
%	\caption{{\bf Performance evaluation of FDR controlling methods.} (A) QQ plot of the EPVs (red dots) and the TNA EPVs (green dots) against the theoretical uniform distribution. The results of the Fisher's combination tests are indicated in the legend. (B) The number of accepted classifications as a function of the Q-values obtained with (i) ground truth (black line), (ii) BH with EPV (red), (iii) BH with TNA EPV (green), and (iv) LTT (blue). (C) Same as (B) but over a critical Q-value range. (D) Deviation of the estimated FDR from the actual FDR obtained with (i) BH with EPV (red line), (ii) BH with EPV and TNA (green line), (iii) LTT (blue line).}
%	\label{fig:pcam}
%\end{figure} 
%\subsection{CheXpert: chest x-ray dataset}
%
%The CheXpert dataset comprises a total of 224,316 radiographs of both frontal and lateral views of the thorax from $\sim$65,000 patients in order to facilitate the automated interpretation of chest X-rays for multi-label classification tasks. We used the deep neural network model, called DenseNet121, from the \texttt{Torchxrayvision} package \cite{cohen2020limits,Cohen2022xrv}, specifically developed for this CheXpert dataset. While CheXpert is of multi-labeled, we turned it to a binary classification in which we treated the Pleural Effusion class as positive and all others as negative. The proportion of the positive class was 27 \% and 23 \% in the training and test datasets. and fine-tuned the DenseNet121 model accordingly, and it achieved 89 \% and 84 \% accuracies on the training and test sets.
%
%We run our evaluation and we found out that all methods: BH with standard EPVs and with TNA EPVs, and the LTT method, provide accurate FDR control. In our opinion, this indicates the absence of any database distribution shits. The also shows that out TNA method does not mess up with the training or test null distributions when there is no data shift.
%
%On the other hand, when diving into the real-world frequency of pleural effusion diagnosis asserted, the percentages fall down drastically. For instance, around 1.5 million of US citizens face pleural effusion annually, making only 0.005% of the entire population. Similar statistics are listed for each developed country, with even lower numbers being presented \cite{Zaki2024}\cite{Cashen2017PleuralEA}. Hence, it can be confidently underscored that dataset's share of pleural effusion patients is only a single distribution, being absolutely erratic to major class disitrbution alterations. Thefore, we introduce the second experiment with CheXpert dataset, where proportion of postiive class is reduced to only 13\%, a two-times fall in general.  
%
%% This comprehensive corpus encompasses both frontal and lateral views of the thorax. The principal objective of this dataset is to facilitate the automated interpretation of chest X-rays for the task of multi-label classification. It is noteworthy that the dataset is enhanced with uncertainty labels, which provide additional context surrounding the confidence levels of the predictions made by automated systems. Furthermore, the CheXpert dataset includes radiologist-labeled reference standard evaluation sets, which serve as a benchmark for assessing the accuracy and reliability of automated interpretation algorithms in comparison to human expert evaluations. The integration of diverse data types and reference standards positions the CheXpert dataset as a significant resource for advancing research and development in medical imaging and machine learning applications.
%
%%As for the selected model, a DenseNet121, being specifically elaborated for the CheXpert dataset as a part of extensive package called 'Torchxrayvision' \cite{cohen2020limits}\cite{Cohen2022xrv}, was chosen. While it initially was designed to solve a task of multilabel classification among xray images, we further finetuned it for classifying 'Pleural Effusion' only. The proportion of positive class within the training set reached 27\%, insignificantly overwhelming the same number for test - 23\%. Similar situation is seen in context of the measured quality: 89\% accuracy against 84\% for train and test respectively. Therefore, no significant shift of data or class proportion rebalance is identified, which in turn might lead to challenges for those models trained when dealing with real-life applications. For instance, in \cite{joseph_d__viviano__2019} authors elaborate on the issue of poor generalisation abilities of algorithms being trained on a specific dataset of xray images and then tested on another, being a straight consequence of data distribution shifts.
%
%\begin{figure}[h!]
%	\centering
% 		\includegraphics[width=1.in]{img/chx.png}
%	\caption{{\bf Instances of CheXpert dataset.}}
%	\label{fig:pcam_example}
%\end{figure} 
%
%
%\begin{figure}[h!]
%	
%	\begin{tabular}{cccc}
%		\multicolumn{4}{l}{\bf (i) Standard BCSS dataset:}\\		
%		\includegraphics[width=1.7in]{img/cnn_QQ_multi_sa_bcss.png} &
% 		\includegraphics[width=2in]{img/cnn_multi_sa_bcss_fdr_control.png} &
%		\includegraphics[width=2in]{img/cnn_multi_sa_bcss_fdr_control_loc.png}  &
%		\includegraphics[width=1.7in]{img/cnn_FDPscat_multi_sa_bcss.png}\\	
%		\multicolumn{4}{l}{\bf (ii) BCSS with class distribution shift.:}\\		
%		\multicolumn{4}{l}{\bf (iii) Standard CheXpert dataset:}\\	
%		\includegraphics[width=1.7in]{img/cnn_QQ_chx.png} 		&
%		\includegraphics[width=1.7in]{img/cnn_chx_fdr_control.png} & 
%        \includegraphics[width=1.7in]{img/cnn_chx_fdr_control_loc.png} & 
%        \includegraphics[width=1.7in]{img/cnn_FDPscat_chx.png}\\
%        \multicolumn{4}{l}{\bf (iii) CheXpert dataset with class distribution shift:}\\	
%		\includegraphics[width=1.7in]{img/cnn_QQ_chx_balanced.png} 		&
%		\includegraphics[width=1.7in]{img/cnn_chx_balanced_fdr_control.png} & 
%        \includegraphics[width=1.7in]{img/cnn_chx_balanced_fdr_control_loc.png} & 
%        \includegraphics[width=1.7in]{img/cnn_FDPscat_chx_balanced.png}\\
%		A & B & C & D
%	\end{tabular}
%	\caption{{\bf Performance evaluation of FDR controling methods.} Plot explanation is same as in Figure \ref{fig:pcam}. }
%	\label{fig:bcss}
%\end{figure} 
%>>>>>>> b44469739e057059bd84d98cf26790a351b4fb81

\section{Conclusions}

In this study, we investigated the effects of data shift on False Discovery Rate (FDR) control. In our opinion, BH procedure with standard EPVs provide very accurate FDR control in the absence of any data shift. Unfortunately, data shift is rather common in practice, and we demonstrated that standard BH and the LTT methods are quite sensitive even to small data of class distribution shift. In this paper, we presented a simple, but robust method, called Test Null Adjustment (TNA), in order to mitigate the effect of the data shift on FDR control. Our method operates in the space of prediction scores, thereby it is not effected by the course of conditionality problem, which often hinders methods that aim to mitigate data shift in the input feature space. The TNA method consists of two main steps. First, it approximates the null distribution among the scores of the test instances via certain statistics from the distributions of the scores of the training instances. Second, it recalibrates the test scores so that the test and train null distributions would move closer to each other. TNA is fully data-driven, it does not rely on any theoretical assumptions or forms of the data. We have demonstrated in our experimental test with four biomedical datasets that NTA can successfully mitigate the bias caused by data shift; while, leaving the distributions intact in the absence of data shift. We note that, handling data shift in practice is hard, and unfortunately our method does not provide any theoretical guarantee that the FDR control with TNA EPVs becomes accurate, or conservatively or liberally biased without any additional assumptions; hence it is a heuristic in its general form.  

With this study, we also aim to emphasize and popularize the importance of accurate error control in any predictions in the biomedical domain. We have shown that the deviations in the simple QQ plots of the calculated vs. the theoretical p-values can be used to detect data shifts between the training and test data. Moreover, we also hope that the FDR control and plot using q-values will become a standard metric in performance evaluation for prediction systems in the biomedical domain.




%\todo{andrey}{rewrite conclusion, add discussion}
%The original exact p-value (XPV) calculation methods for scoring tandem mass spectrometry data using a dot-product-like scoring function were introduced for low-resolution fragmentation settings almost 15 years ago. Until now, it has remained an open question whether this algorithm can be made suitable for high resolution fragmentation settings. In this article we showed a generalization of the XPV method, termed HR-XPV, which is capable of producing accurate and well calibrated exact p-values for XCorr scores obtained with scoring high resolution MS/MS data. Unfortunately, our solution can be considered rather slow, which raise questions about its practicality. However, we hope that, the ideas in HR-XPV presented might be a significant step toward the development of efficient methods for calibrating exact p-values for high resolution MS/MS data. 

\section*{Author contributions}
\ifdefined\DOUBLEBLINDREVIEW
Omitted for double-blind peer-review.
\else
AKF concieved the idea that accurate empirical p-values would be derived from training data. AB worked out the methods and carried out experiments. AKF and AB wrote the manuscript.
\fi

%\printbibliography	 
%\bibliographystyle{plain}           % Style BST file.
\bibliographystyle{unsrt}           % Style BST file.
\bibliography{bibliography}

\end{document}
