\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{color}
\usepackage{graphicx}
\usepackage{multirow} % Allow multirow entries in tables.
\usepackage[sort&compress,super]{natbib}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{siunitx}
\usepackage{geometry}

\newcommand{\response}[1]{\vspace*{1ex} \color{blue} \noindent #1 \color{black}
\vspace*{2ex}}

\newcommand{\edit}[1]{\begin{quotation}\color{red}\noindent #1
\color{black}\end{quotation}}

\newcommand{\myinput}[1]{\color{red}\input{#1}\color{black}}

\newcommand{\fixme}[1]{{\color{red} FIXME: #1 }}

% Make a code block.
\newcommand{\codeblock}[1]{{\small \color{cyan}\begin{verbatim}{{#1}}\end{verbatim}}}

\begin{document}
	
\noindent
\today\\[2ex]

\noindent
Dear Dr.\ Alessandro Barbiero:\\[2ex]

We thank you and the reviewers for your kind consideration of our manuscript.
Below, we address each of the points raised by the three reviewers and describe the changes we have made to the manuscript.
In what follows, the reviewer's comments are shown in black type, interleaved with our responses (in blue) and, where appropriate, the modified text (in red).
We have also made the formatting changes that you requested.

In general, the reviewers have pointed out few issues with the text. We also think ...

Thank you very much for your consideration.\\[2ex]

\noindent
Best regards,\\[2ex]

\noindent
Attila Kertesz-Farkas\\
HSE University

\clearpage
\section*{Reviewer 1}

This paper incorporates the proportion of the null hypothesis to adjust the distribution shift from training to testing data in order to control the FDR. 

\response{Thank you for this accurate summary of our work.}

I found the paper not well presented and it is very difficult to follow the logic flow. I have the following questions for the authors to consider:

1. It is not transparent to me regarding the rationale of the proposed method. The goal of multiple testing is to separate the non-null from the null. How can we estimate the density function of null and non-null without knowing which one is from which? Perhaps we can use the empirical distribution of the test statistic, assuming the null distribution, to derive the alternative distribution as that in Gao and Zhao (2024).

\response{We are very grateful for the reviewer for pointing out this issue. We approached this problem entirely from practical point of view during manuscript preparation, and we did not consider readers from the theoretical side. We have added a long paragraph to the introduction that summarizes the problem statement and our method in a more formal way. It now reads as follows.}

\edit{Formally speaking, we are }

2. Many methods are proposed to estimate the proportion of null hypothesis, for instance, the ones proposed in Meinshausen and Rice (2006); Wang, H.-Q. et al. (2011). In Wang, H.-Q. et al. (2011), the method adjusts for dependence, which should be particularly relevant to the applications con- sidered in this paper. Can the authors try these approaches and compare the results?
\response{We added the following text to the manuscript:}

\edit{Todo:}

3. Furthermore, it is assumed that the variance of training and testing are the same, which is a strong assumption. Is there any way to relax this?


\response{We added the following text to the manuscript:}

\edit{Todo:}

4. In practice, how do you determine the number of histogram bins and how many bins we should use?

\response{We added the following text to the manuscript:}

\edit{Todo:}

\begin{table}[htbp]
\centering
\caption{$\pi_0$ estimation for the TissueNet dataset. Real train proportion: 0.407.}
\label{tab:tissuenet}
\begin{tabular}{l *{9}{S[table-format=1.3]}}
\toprule
& \multicolumn{9}{c}{Methods} \\
\cmidrule(lr){2-10}
& {Our} & {Storey} & {Meinshausen} & {Jiang} & {Nettleton} & {Slim} & {Pounds} & {Last hist} & {Real test} \\
\midrule
Standard & 0.318 & 0.269 & 0.716 & 0.289 & 0.303 & 0.355 & 0.316 & 0.257 & 0.339 \\
\midrule
Rebalanced & 0.950 & 0.950 & 0.284 & 0.858 & 0.714 & 0.724 & 0.808 & 0.744 & 0.668 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{$\pi_0$ estimation for the Chexpert dataset. Real train proportion: 0.745.}
\label{tab:chexpert}
\begin{tabular}{l *{9}{S[table-format=1.3]}}
\toprule
& \multicolumn{9}{c}{Methods} \\
\cmidrule(lr){2-10}
& {Our} & {Storey} & {Meinshausen} & {Jiang} & {Nettleton} & {Slim} & {Pounds} & {Last hist} & {Real test} \\
\midrule
Standard & 0.779 & 0.760 & 0.184 & 0.779 & 0.785 & 0.861 & 0.809 & 0.763 & 0.749 \\
\midrule
Rebalanced & 0.897 & 0.881 & 0.087 & 0.891 & 0.890 & 0.925 & 0.902 & 0.878 & 0.870 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{$\pi_0$ estimation for the PCAM dataset. Real train proportion: 0.509.}
\label{tab:pcam}
\begin{tabular}{l *{9}{S[table-format=1.3]}}
\toprule
& \multicolumn{9}{c}{Methods} \\
\cmidrule(lr){2-10}
& {Our} & {Storey} & {Meinshausen} & {Jiang} & {Nettleton} & {Slim} & {Pounds} & {Last hist} & {Real test} \\
\midrule
Standard & 0.572 & 0.580 & 0.422 & 0.569 & 0.564 & 0.626 & 0.583 & 0.575 &  0.503 \\
\midrule
Rebalanced & 0.937 & 0.967 & 0.128 & 0.904 & 0.863 & 0.876 & 0.887 & 0.953 &  0.835 \\
\bottomrule
\end{tabular}
\end{table}

\section*{Reviewer 2}
1. The proposed TNA method aims to address the issue of distribution shifts between the calibration data and target data in conformal classification.
However, distribution shifts can manifest in various forms, including covariate shift, class-wise covariate shift, label shift, or posterior shift.
I suggest the authors clarify which specific types of distribution shifts the proposed method can handle, and provide the rationale for the effectiveness.

\response{We added the following text to the manuscript:}

\edit{Todo:}

2. The aim of the article is to provide false discovery rate control in the presence of distribution shift.
However, with the proposed TNA method, it remains unclear to me whether the FDR is controlled with respect to the distribution of target data or the calibration data, as the target scores are adjusted to align with the calibration scores in TNA.
I suggest the authors provide a discussion on this potential issue.

\response{We added the following text to the manuscript:}

\edit{Todo:}

3. The quantity $\hat{\pi}_0$ appears in TNA+ method, but it causes confusion with the Storey null proportion estimator in the final BH procedure.
The authors should provide more detailed explanation on how to obtain $\hat{\pi}_0$ in step 3.b.

\response{We added the following text to the manuscript:}

\edit{Todo:}

4. The introduced LTT in Section 2.1 is misleading, as the primary focus of LTT in Angelopoulos et al. (2021, arxiv) differs from the false discovery rate control that is essentially related with the BH algorithm.
Besides, the use of notation $\lambda$ in LTT causes confusion with the $\lambda$ in Storey null proportion estimator.
I suggest the authors provide a clearer explanation on the LTT if it is necessary for the current problem.
\response{We added the following text to the manuscript:}

\edit{Todo:}

\section*{Reviewer 3}

The authors propose a new method for the false discovery rate control in classification problems. They adjust the test prediction scores so that the test null distribution
aligns to the training null distribution, and more accuarte p-vlaues of the test samples can be computed.

Question:

1.TNA- (or TNA+) assumes that $T_0[i]/T_s[i]= X_0[i]/X_S[i] (or T_1[i]/T_s[i]= X_1[i]/X_S[i] )$. Why is this assumption made? Do the authors assume that the proportion of the true null hypothesis remains constant between the training and test samples?"

\response{We added the following text to the manuscript:}

\edit{Todo:}


\bibliographystyle{unsrt}
\bibliography{refs} % See https://github.com/Noble-Lab/noble-lab-references

\end{document}
